{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn CNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add conv layer\n",
    "add l2 regularizer\n",
    "add LSTMBlockFused\n",
    "try LSTMcudnn\n",
    "remove dropout to add optimized graph, add quantized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 1.10.1 of tensorflow\n",
      "shape input train (43951, 150)\n",
      "Data loaded\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('Codes')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dataShaping import *\n",
    "from savePerf import *\n",
    "from saveTransformedGraph import optimizeGraph\n",
    "import scipy.io.wavfile\n",
    "import time\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelName = \"CNN1\"\n",
    "# create directory experiment\n",
    "date = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "path = os.path.join(\"Experiments\",date)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "    #experiment/\"date\"/temp will contain the backuped model parameters\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "    # if you run the file two time in a minute\n",
    "else :\n",
    "    date = date+'(2)'\n",
    "    path = os.path.join(\"Experiments\",date)\n",
    "    os.makedirs(path)\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "\n",
    "# directory that will contain tensorboard information\n",
    "pathLog = 'Tf_logs'\n",
    "if not os.path.isdir(pathLog):\n",
    "    os.makedirs(pathLog)\n",
    "pathLog = \"{}/run-{}/\".format(pathLog,date)\n",
    "\n",
    "version = tf.__version__\n",
    "print (\"version {} of tensorflow\".format(version))\n",
    "\n",
    "#############################\n",
    "# Model parameters\n",
    "#############################\n",
    "trainTestRatio = 0.8\n",
    "#if you cannot load all the data set in Ram specify wich part you want to load (0 means all the dataset)\n",
    "maxSize = 44100\n",
    "num_step = 150                                       # time step before reduction\n",
    "conv_chan = [35 , 70]                                #number of kernel for convolution\n",
    "conv_strides = 35                                    #delay between two convolution\n",
    "conv_size = 12                                       #filter size for the convolution\n",
    "size_poll = 4\n",
    "reg_scale = 0\n",
    "l1l2Prop = 0.4  # 1 =>l1, 0=> l2\n",
    "reg_scale_l1 = l1l2Prop*reg_scale\n",
    "reg_scale_l2 = ((1-l1l2Prop)/2)*reg_scale\n",
    "num_hidden = 200                                      #num of hidden units\n",
    "num_class = 1                                         #size of the output\n",
    "num_feature = 1                                       # size of the input\n",
    "batch_size = 2500                                     # number of sequence taken before to compute the gradient\n",
    "n_layer = 1                                           #num_layer\n",
    "\n",
    "#num_hidden = num_hidden/keep_prob\n",
    "num_epoch = 10000                                    # process all the datas num_epoch times\n",
    "trainDuration = 60*60*15                              # or during a determined duration(second)\n",
    "amplifierName = 'EnglDisto'\n",
    "fileNameTrain = 'Datasets/training'+amplifierName+'.mat'             #dataset train/test path\n",
    "fileNameTest = 'Datasets/test'+amplifierName+'.mat' # dataset validation path\n",
    "fileNameValidation = 'Datasets/val'+amplifierName+'.mat'\n",
    "\n",
    "#############################\n",
    "# Loading data\n",
    "#############################\n",
    "matrix = sio.loadmat(fileNameTrain)\n",
    "matrixTrain = matrix['train']\n",
    "matrix = sio.loadmat(fileNameTest)\n",
    "matrixTest = matrix['test']\n",
    "if maxSize ==0:\n",
    "    maxSize = len(matrixTrain)\n",
    "    print(\"maxSize = {}\".format(maxSize))\n",
    "\n",
    "train_input,train_output,test_input,test_output = loadInputOutputSeq(matrixTrain,matrixTest,num_step,maxSize)\n",
    "\n",
    "print(\"shape input train {}\".format(np.shape(train_input)))\n",
    "numTrain = len(train_output)\n",
    "print (\"Data loaded\")\n",
    "#######################\n",
    "#Graph\n",
    "#######################\n",
    "\n",
    "G = tf.Graph()\n",
    "with G.as_default():\n",
    "    with tf.name_scope(\"placeHolder\"):\n",
    "        data = tf.placeholder(tf.float32, [None, num_step], name =\"data\") #Number of examples, number of input step (time step), dimension of each input\n",
    "        target = tf.placeholder(tf.float32, [None, num_class],name = \"target\") # batchSize, nbClass\n",
    "     \n",
    "    dataShaped = tf.reshape(data,[tf.shape(data)[0],num_step,1,1]) # batchSize, height,width,channel  \n",
    "    dataShaped = tf.transpose(dataShaped,[0,3,1,2]) # faster on gpu btachSize,Channel,Height,width\n",
    "    \n",
    "    with tf.variable_scope(\"ConvLayers\"):\n",
    "        regularizerC1 = tf.contrib.layers.l1_l2_regularizer(scale_l1=reg_scale_l1,scale_l2=reg_scale_l2,scope=\"regC1\")\n",
    "        conv1 = tf.layers.conv2d(inputs = dataShaped,filters = conv_chan[0],\n",
    "                                       kernel_size = (conv_size,1),strides=(3,1),\n",
    "                                       padding = \"same\",activation=tf.nn.relu,kernel_regularizer=regularizerC1,\n",
    "                                       data_format='channels_first',name=\"conv1\")\n",
    "        \n",
    "        # pool1\n",
    "        pool1 = tf.nn.max_pool(conv1, ksize=[1, 1, 3, 1], strides=[1, 1, 2, 1],data_format='NCHW',padding='SAME', name='pool1')  \n",
    "        regularizerC2 = tf.contrib.layers.l1_l2_regularizer(scale_l1=reg_scale_l1,scale_l2=reg_scale_l2,scope=\"regC2\")\n",
    "        conv2 = tf.layers.conv2d(inputs = pool1,filters = conv_chan[1],\n",
    "                                       kernel_size = (conv_size/2,1),strides=(1,1),\n",
    "                                       padding = \"same\",activation=tf.nn.relu,kernel_regularizer=regularizerC1,\n",
    "                                       data_format='channels_first',name=\"conv2\")\n",
    "        \n",
    "\n",
    "        # pool2\n",
    "        pool2 = tf.nn.max_pool(conv2, ksize=[1, 1, 3, 1], strides=[1, 1, 2, 1],data_format='NCHW',padding='SAME', name='pool2')\n",
    "        \n",
    "    # reshape pool 2 cnn to make a fully connected layer entry\n",
    "    dataShape = int(np.prod(pool2.get_shape()[1:]))\n",
    "    dataReshaped = tf.reshape(pool2, [-1, dataShape])\n",
    "    FC1 = fully_connected(dataReshaped,1000,activation_fn=tf.nn.relu,weights_regularizer=None,scope=\"FC1\")\n",
    "\n",
    "    prediction = fully_connected(FC1,int(target.get_shape()[1]),activation_fn=tf.nn.tanh,weights_regularizer=None,scope=\"FCPred\")\n",
    "\n",
    "    #Compute the mean square error\n",
    "    MSE = tf.reduce_mean(tf.square(prediction-target))\n",
    "    EnergyTarget = tf.reduce_mean(tf.square(target)) \n",
    "    \n",
    "    #get regularizer\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    MSEReg = tf.add_n([MSE]+reg_losses,name=\"MSEReg\")\n",
    "    # create optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    #Compute gradient and apply backpropagation\n",
    "    minimize = optimizer.minimize(MSEReg)\n",
    "\n",
    "    # Create summary view for tensorboard\n",
    "    mse_summary = tf.summary.scalar('RMSE',tf.sqrt(MSE))\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    #Create an init op to initialize variable\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() # save variable, use saver.restore(sess,\"date/tmp/my_model.ckpt\") instead of sess.run(init_op)\n",
    "\n",
    "##############################\n",
    "# Execution du graphe\n",
    "##############################\n",
    "    \n",
    "with tf.Session(graph=G) as sess:\n",
    "    #restorePath = os.path.join('2017-09-11-18-07','temp','my_model.ckpt')\n",
    "    #saver.restore(sess,restorePath)\n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(pathLog+'train',graph =tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(pathLog+'test')\n",
    "    \n",
    "    no_of_batches = int(np.floor((numTrain)/batch_size)) # numtrain -numstep    no_of_batchesTest = int(np.floor((len(test_input)-num_step)/batch_size))\n",
    "    no_of_batchesTest = int(np.floor((len(test_input))/batch_size))\n",
    "    tStart = time.clock()\n",
    "    epoch =0\n",
    "    NRMSETest = 10\n",
    "    bestNRMSETest = 1    \n",
    "    # train until the number of epoch or the training time is reached\n",
    "    for epoch in range(num_epoch):\n",
    "        tEpoch = time.clock()\n",
    "        if (time.clock()-tStart < trainDuration) :\n",
    "            ptr = 0\n",
    "            if epoch % 20==0 : # each twenty epochs save the model\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myGraph.pb',as_text=False)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myModel.ckpt'))\n",
    "            \n",
    "            pMSETrain=0\n",
    "            pEnergyTarget=0\n",
    "            for j in range(no_of_batches):\n",
    "                inp, out = train_input[ptr:ptr+batch_size],train_output[ptr:ptr+batch_size]\n",
    "                ptr+=batch_size\n",
    "                \n",
    "                if j % np.floor(numTrain/len(test_input)) ==0 : # This is to have a train summary and a test summary of the same size\n",
    "                    _,summary_str,pMSETrainTemp,pEnergyTargetTemp,p = sess.run([minimize,summary_op,MSE,EnergyTarget,tf.shape(pool2)],{data: inp, target: out})\n",
    "                    print(\"shape:{}\".format(p))\n",
    "                    pMSETrain += pMSETrainTemp\n",
    "                    pEnergyTarget += pEnergyTargetTemp\n",
    "                    step = epoch*no_of_batches+j\n",
    "                    # save the training RMSE for tensorboard\n",
    "                    train_writer.add_summary(summary_str,step)                   \n",
    "\n",
    "                else :\n",
    "                    _,pMSETrainTemp,pEnergyTargetTemp = sess.run([minimize,MSE,EnergyTarget],{data: inp, target: out})\n",
    "                    pMSETrain += pMSETrainTemp\n",
    "                    pEnergyTarget += pEnergyTargetTemp\n",
    "                    #[print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "            # compute an estimation of the RMSE for this epoch       \n",
    "            MSETrain = pMSETrain/no_of_batches\n",
    "            EnergyTargetTrain = pEnergyTarget/no_of_batches\n",
    "            NRMSETrain = np.sqrt(MSETrain/EnergyTargetTrain)\n",
    "            print (\"Epoch -{} calculated in {:5.2f} s \".format(epoch,time.clock()-tEpoch))\n",
    "            # evaluate the model on the test set \n",
    "            pMSE = 0\n",
    "            ptr2 = 0\n",
    "            pEnergyTarget = 0\n",
    "            for k in range(no_of_batchesTest):\n",
    "                pMSETemp,pEnergyTargetTemp,summary_str = sess.run([MSE,EnergyTarget,summary_op],{data: test_input[ptr2:ptr2+batch_size] , target: test_output[ptr2:ptr2+batch_size]})\n",
    "                pMSE += pMSETemp\n",
    "                ptr2 += batch_size\n",
    "                pEnergyTarget+=pEnergyTargetTemp\n",
    "                step = epoch*no_of_batchesTest+k\n",
    "                test_writer.add_summary(summary_str,step*np.floor(numTrain/len(test_input)))\n",
    "            MSETest = pMSE/no_of_batchesTest\n",
    "            EnergyTargetTest = pEnergyTarget/no_of_batchesTest\n",
    "            NRMSETest = np.sqrt(MSETest/EnergyTargetTest)\n",
    "            if NRMSETest<bestNRMSETest:\n",
    "                bestNRMSETest=NRMSETest\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myBestGraph.pbtxt',as_text=True)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myBestModel.ckpt'))\n",
    "            print(\"Epoch {}, NRMSE Test/best: {:.5f}/{:.5f}, Training NRMSE: {:.5f}, deviation of {:.2f}%\".format(epoch,NRMSETest,bestNRMSETest,NRMSETrain,100*np.sqrt((NRMSETrain-NRMSETest)**2)/NRMSETrain))\n",
    "        else : break # break the while loop if number of epoch is reached\n",
    "    tStop = time.clock()\n",
    "    trainTime = time.strftime(\"%d:%H:%M:%S \", time.gmtime(tStop-tStart))\n",
    "   \n",
    "    #######################\n",
    "    # Save Graph variable and information about the running session\n",
    "    #######################\n",
    "    # save graph model\n",
    "    tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myFinalGraph.pbtxt',as_text=True)\n",
    "    # Save checkpoint variables\n",
    "    save_path = saver.save(sess,os.path.join(pathTemp,'myFinalModel.ckpt'))\n",
    "    print (\"Training duration {}\".format(trainTime))\n",
    "    totalParameters =np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()])\n",
    "    print(\"Number of training variable {}\".format(totalParameters))\n",
    "    # log\n",
    "    infoLog={}\n",
    "    infoLog[\"path\"] = path\n",
    "    infoLog[\"MSE\"] = bestNRMSETest\n",
    "    infoLog[\"num_step\"] = num_step\n",
    "    infoLog[\"num_hidden\"] = num_hidden\n",
    "    infoLog[\"num_epoch\"] = epoch\n",
    "    infoLog[\"batch_size\"] = batch_size\n",
    "    infoLog[\"maxSize\"] = maxSize\n",
    "    infoLog[\"duration\"] = trainTime\n",
    "    infoLog[\"totalParameters\"] = totalParameters\n",
    "    infoLog[\"version\"] = version\n",
    "    infoLog[\"n_layer\"] = n_layer\n",
    "    infoLog[\"trainDropout\"] = 0\n",
    "    infoLog[\"nameModel\"] = modelName\n",
    "    infoLog[\"conv_chan\"] = conv_chan\n",
    "    infoLog[\"strides\"] = conv_strides\n",
    "    infoLog[\"conv_size\"] = conv_size\n",
    "    infoLog[\"amplifierName\"]=amplifierName\n",
    "    logPerf(infoLog)\n",
    "    input_nodes=[\"placeHolder/data\"]\n",
    "    output_nodes=[\"FCPred/Tanh\"]\n",
    "    optimizeGraph(pathTemp,input_nodes,output_nodes)\n",
    "    \n",
    "    \n",
    "                                                 \n",
    "    \n",
    "    ###############################\n",
    "    #   validation dataset and emulate guitar signal\n",
    "    ###############################\n",
    "    matrixVal = sio.loadmat(fileNameValidation)\n",
    "    matrixVal = matrixVal['val']  \n",
    "    valSize = 0\n",
    "    if valSize == 0 :\n",
    "        valSize = len(matrixVal)\n",
    "    # shape validation test\n",
    "    val_input,val_output = loadValidationSeq(matrixVal,num_step,valSize)\n",
    "    lPrediction = []\n",
    "    lTarget = []\n",
    "    ptr3 = 0\n",
    "    no_of_batchesVal = int(np.floor((len(val_input))/batch_size))\n",
    "    for k in range(no_of_batchesVal):\n",
    "        pPrediction,pTarget = sess.run([prediction,target],{data: val_input[ptr3:ptr3+batch_size], target: val_output[ptr3:ptr3+batch_size]}) \n",
    "        lPrediction.append(pPrediction)\n",
    "        lTarget.append(pTarget)   \n",
    "        ptr3+=batch_size\n",
    "    #plt.show()scree\n",
    "    predictionArray = np.array(lPrediction,dtype=np.float32).ravel()\n",
    "    targetArray = np.array(lTarget,dtype=np.float32).ravel()\n",
    "    scipy.io.wavfile.write(os.path.join(path,'prediction.wav'),44100,predictionArray)\n",
    "    scipy.io.wavfile.write(os.path.join(path,'target.wav'),44100,targetArray)\n",
    "\n",
    "    # save emulation in a pickle format\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(predictionArray[6000+num_step:6700+num_step],label='prediction')\n",
    "    ax.plot(targetArray[6000+num_step:6700+num_step],label='target')\n",
    "    ax.legend()\n",
    "    plt.xlabel('sample n')\n",
    "    plt.ylabel('Amplitude y[n]')\n",
    "    nameFigEstimation = os.path.join(path,\"targetVsPrediction.pickle\")\n",
    "    pickle.dump(ax,open(nameFigEstimation, 'wb'))\n",
    "print (\"done, good job kids\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from show import *\n",
    "%matplotlib notebook\n",
    "showPickle(nameFigEstimation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
