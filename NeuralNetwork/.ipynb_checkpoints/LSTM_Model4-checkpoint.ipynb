{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM_model4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM_model4 use two input features to take into account the gain parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 1.13.1 of tensorflow\n",
      "maxSize = 33632035\n",
      "NumTrain: 33632035, NumTest:8408015\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/rnn/python/ops/lstm_ops.py:694: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch -0 calculated in 1260.43 s \n",
      "Epoch 0, NRMSE Test/best: 1.53352/1.53352, Training NRMSE: 0.44316, deviation of 246.04%\n",
      "Training duration 01:00:22:00 \n",
      "Number of training variable 91951\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/freeze_graph.py:249: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from Experiments/2019-05-07-10-46/temp/myFinalModel.ckpt\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.convert_variables_to_constants\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.extract_sub_graph\n",
      "INFO:tensorflow:Froze 4 variables.\n",
      "INFO:tensorflow:Converted 4 variables to const ops.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/tools/optimize_for_inference_lib.py:113: remove_training_nodes (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.compat.v1.graph_util.remove_training_nodes\n",
      "INFO:tensorflow:Restoring parameters from Experiments/2019-05-07-10-46/temp/myBestModel.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fileNameValidation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d4f8c854c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mrestorePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathTemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'myBestModel.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# example for restore a previous model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrestorePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     \u001b[0mmatrixVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileNameValidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0mmatrixVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrixVal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0mvalSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fileNameValidation' is not defined"
     ]
    }
   ],
   "source": [
    "#### !/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"lstm for guitar signal\"\"\"\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('Codes')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dataShapingGain import *\n",
    "from savePerf import *\n",
    "import scipy.io.wavfile\n",
    "import time\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import *\n",
    "from saveTransformedGraph import optimizeGraph\n",
    "\n",
    "modelName = \"LSTM_model4\"\n",
    "\n",
    "#############################\n",
    "# Directory experiment\n",
    "#############################\n",
    "date = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "path = os.path.join(\"Experiments\",date)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "    #experiment/\"date\"/temp will contain the backuped model parameters\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "    # if you run the file two time in a minute\n",
    "else :\n",
    "    date = date+'(2)'\n",
    "    path = os.path.join(\"Experiments\",date)\n",
    "    os.makedirs(path)\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "\n",
    "# directory that will contain tensorboard information\n",
    "pathLog = 'Tf_logs'\n",
    "if not os.path.isdir(pathLog):\n",
    "    os.makedirs(pathLog)\n",
    "pathLog = \"{}/run-{}/\".format(pathLog,date)\n",
    "\n",
    "version = tf.__version__\n",
    "print (\"version {} of tensorflow\".format(version))\n",
    "\n",
    "#############################\n",
    "# Model parameters\n",
    "#############################\n",
    "trainTestRatio = 0.8\n",
    "maxSize = 0\n",
    "num_subBatchesTrain = 5 # if dataset does not fit in the memory we work with subatches of size batch/subsize\n",
    "num_subBatchesTest = 3\n",
    "num_step = 100\n",
    "num_hidden = 150\n",
    "num_class = 1\n",
    "num_feature = 2\n",
    "batch_size = 1000\n",
    "num_epoch = 1                                     # process all the datas num_epoch times\n",
    "trainDuration = 60*60*24                            # or during a determined duration(second)\n",
    "                                      \n",
    "amplifierName = 'EnglDisto_Gain1-5'\n",
    "fileNameTrain = 'Datasets/training'+amplifierName+'.mat'             #dataset train/test path\n",
    "fileNameTest = 'Datasets/test'+amplifierName+'.mat' # dataset validation path\n",
    "fileNameValidation = 'Datasets/valEnglDisto.mat'\n",
    "\n",
    "#############################\n",
    "# Loading data\n",
    "#############################\n",
    "matrix = sio.loadmat(fileNameTrain)\n",
    "matrixTrain = matrix['train']\n",
    "matrix = sio.loadmat(fileNameTest)\n",
    "matrixTest = matrix['test']\n",
    "#matrixTest=matrixTest[:44100,:]\n",
    "if maxSize ==0:\n",
    "    maxSize = len(matrixTrain)\n",
    "    print(\"maxSize = {}\".format(maxSize))\n",
    "#train_input,train_output,test_input,test_output = loadInputOutputGain(matrixTrain,matrixTest,num_step,maxSize)\n",
    "#print(\"shape input train {}\".format(np.shape(train_input)))\n",
    "numTrain = maxSize\n",
    "numTest = len(matrixTest)\n",
    "print (\"NumTrain: {}, NumTest:{}\".format(numTrain,numTest))\n",
    "\n",
    "#######################\n",
    "# Graph Construction\n",
    "#######################\n",
    "G = tf.Graph()\n",
    "with G.as_default():\n",
    "    with tf.name_scope(\"placeHolder\"):\n",
    "        data = tf.placeholder(tf.float32, [None, num_step,num_feature], name =\"data\") #minibatch size, number of input step (time step), dimension of each input\n",
    "        target = tf.placeholder(tf.float32, [None, num_class],name = \"target\") # minibatch size, nbClass\n",
    "        dataShaped = tf.transpose(data,[1,0,2])# [num_step,minibatch_size,num_feature]\n",
    "    \n",
    "    with tf.name_scope(\"LSTMLayer\"):\n",
    "        fusedCell = tf.contrib.rnn.LSTMBlockFusedCell(num_hidden,use_peephole=False)\n",
    "        val, state = fusedCell(dataShaped,dtype=tf.float32) #c, h DIM [numstep,minibatch size]\n",
    "        \n",
    "    with tf.name_scope(\"extractLastCelloftheLSTMLayer\"):\n",
    "        # Let's first fetch the last index of seq length\n",
    "        # last_index would have a scalar value\n",
    "        last_index = tf.shape(val)[0] - 1\n",
    "        # Then let's reshape the output to [sequence_length,batch_size,numhidden]\n",
    "        # Last state of all batches\n",
    "        lastState = tf.gather(val,last_index)#[minibatchsize,num_hidden]\n",
    "    \n",
    "    prediction = fully_connected(lastState,int(target.get_shape()[1]),activation_fn=tf.nn.tanh,weights_regularizer=None,scope=\"FCPred\")\n",
    "    \n",
    "##############################\n",
    "# Cost function\n",
    "##############################\n",
    "    MSE = tf.reduce_mean(tf.square(prediction-target))\n",
    "    EnergyTarget = tf.reduce_mean(tf.square(target)) \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    minimize = optimizer.minimize(MSE)\n",
    "    # Create summary view for tensorboard\n",
    "    mse_summary = tf.summary.scalar('RMSE',tf.sqrt(MSE))\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    #Create an init op to initialize variable\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() # save variable, use saver.restore(sess,\"date/tmp/my_model.ckpt\") instead of sess.run(init_op)\n",
    "\n",
    "##############################\n",
    "# Execution du graphe\n",
    "##############################\n",
    "   \n",
    "with tf.Session(graph=G) as sess:\n",
    "    #restorePath = os.path.join('2017-09-11-18-07','temp','my_model.ckpt')\n",
    "    #saver.restore(sess,restorePath)\n",
    "    \n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(pathLog+'train',graph =tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(pathLog+'test')\n",
    "    no_of_batches = int(np.floor((numTrain)/batch_size)) # numtrain -numstep    no_of_batchesTest = int(np.floor((len(test_input)-num_step)/batch_size))\n",
    "    no_of_batchesTest = int(np.floor((numTest)/batch_size))\n",
    "    tStart = time.clock()\n",
    "    epoch =0\n",
    "    NRMSETest = 1\n",
    "    bestNRMSETest = 10   \n",
    "     # train until the number of epoch or the training time is reached\n",
    "    for epoch in range(num_epoch):\n",
    "        tEpoch = time.clock()\n",
    "        if (time.clock()-tStart < trainDuration) :\n",
    "            if epoch % 20==0 : # each twenty epochs save the model\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myGraph.pb',as_text=False)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myModel.ckpt'))\n",
    "        #Shuffle the train matrix between each epoch\n",
    "            #############\n",
    "            # Training\n",
    "            ############# \n",
    "            pMSETrain=0\n",
    "            pEnergyTarget=0\n",
    "            index = 0 # index of subbatches\n",
    "            sizeSubTrain = int(np.floor(no_of_batches/num_subBatchesTrain)) # length of subbatches\n",
    "            numBatchEff=0\n",
    "            for j in range(no_of_batches):\n",
    "                #print(\"batch {}/{}\".format(j,no_of_batches))\n",
    "                if int(j%np.ceil(no_of_batches/num_subBatchesTrain))==0 : # divide no_of_batches in subsize subBatches\n",
    "                    train_input,train_output = loadInputOutputGain2(matrixTrain[(sizeSubTrain)*index*batch_size:batch_size*((sizeSubTrain)*index+sizeSubTrain)-1,:],num_step)\n",
    "                    \n",
    "                    index+=1\n",
    "                    ptr=0               \n",
    "                if ((ptr+batch_size) < batch_size*sizeSubTrain-1):\n",
    "                    inp, out = train_input[ptr:ptr+batch_size],train_output[ptr:ptr+batch_size]\n",
    "                    ptr+=batch_size\n",
    "                    numBatchEff+=1\n",
    "                    #print(\"numBatchEff: {}\".format(numBatchEff))\n",
    "                    if j % np.floor(numTrain/numTest) ==0 : # This is to have a train summary and a test summary of the same size\n",
    "                        _,summary_str,pMSETrainTemp,pEnergyTargetTemp = sess.run([minimize,summary_op,MSE,EnergyTarget],{data: inp, target: out})\n",
    "                        #print(\"shape:{}\".format(numTrain/len(test_input)))\n",
    "                        pMSETrain += pMSETrainTemp\n",
    "                        pEnergyTarget += pEnergyTargetTemp\n",
    "                        step = epoch*no_of_batches+j\n",
    "                        # save the training RMSE for tensorboard\n",
    "                        train_writer.add_summary(summary_str,step)                   \n",
    "                    else :\n",
    "                        _,pMSETrainTemp,pEnergyTargetTemp = sess.run([minimize,MSE,EnergyTarget],{data: inp, target: out})\n",
    "                        pMSETrain += pMSETrainTemp\n",
    "                        pEnergyTarget += pEnergyTargetTemp\n",
    "                        #[print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "            # compute an estimation of the RMSE for this epoch       \n",
    "            MSETrain = pMSETrain/numBatchEff\n",
    "            EnergyTargetTrain = pEnergyTarget/numBatchEff\n",
    "            NRMSETrain = np.sqrt(MSETrain/EnergyTargetTrain)\n",
    "            print (\"Epoch -{} calculated in {:5.2f} s \".format(epoch,time.clock()-tEpoch))\n",
    "            #############\n",
    "            # Testing\n",
    "            #############\n",
    "            pMSE = 0\n",
    "            pEnergyTarget = 0\n",
    "            index = 0\n",
    "            sizeSubTest = int(np.floor(no_of_batchesTest/num_subBatchesTest))\n",
    "            numBatchEff=0 # count the number of minibatch since some of them are too short they are droped\n",
    "            for k in range(no_of_batchesTest):\n",
    "                if int(k%np.ceil(no_of_batchesTest/num_subBatchesTest))==0 : # divide no_of_batches in subsize subBatches\n",
    "                    test_input,test_output = loadInputOutputGain2(matrixTest[(sizeSubTest)*index*batch_size:batch_size*((sizeSubTest)*index+sizeSubTest)-1,:],num_step)\n",
    "                    index+=1\n",
    "                    ptr2=0               \n",
    "                if ((ptr2+batch_size) < batch_size*sizeSubTest-1):\n",
    "                    inptest, outptest = test_input[ptr2:ptr2+batch_size],test_output[ptr2:ptr2+batch_size]                \n",
    "                    pMSETemp,pEnergyTargetTemp,summary_str = sess.run([MSE,EnergyTarget,summary_op],{data: inptest , target: outptest})\n",
    "                    pMSE += pMSETemp\n",
    "                    ptr2 += batch_size\n",
    "                    pEnergyTarget+=pEnergyTargetTemp\n",
    "                    step = epoch*no_of_batchesTest+k\n",
    "                    test_writer.add_summary(summary_str,step*np.floor(numTrain/len(test_input)))\n",
    "                    numBatchEff+=1\n",
    "                \n",
    "            MSETest = pMSE/numBatchEff\n",
    "            EnergyTargetTest = pEnergyTarget/numBatchEff\n",
    "            NRMSETest = np.sqrt(MSETest/EnergyTargetTest)\n",
    "            if NRMSETest<bestNRMSETest:\n",
    "                bestNRMSETest=NRMSETest\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myBestGraph.pbtxt',as_text=True)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myBestModel.ckpt'))\n",
    "            print(\"Epoch {}, NRMSE Test/best: {:.5f}/{:.5f}, Training NRMSE: {:.5f}, deviation of {:.2f}%\".format(epoch,NRMSETest,bestNRMSETest,NRMSETrain,100*np.sqrt((NRMSETrain-NRMSETest)**2)/NRMSETrain))\n",
    "                    \n",
    "                \n",
    "        else : break # break the while loop if number of epoch is reached\n",
    "    tStop = time.clock()\n",
    "    trainTime = time.strftime(\"%d:%H:%M:%S \", time.gmtime(tStop-tStart))\n",
    "    \n",
    "    ################################################################\n",
    "    # Save Graph variable and information about the running session\n",
    "    ################################################################\n",
    "    # save graph model\n",
    "    tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myFinalGraph.pbtxt',as_text=True)\n",
    "    # Save checkpoint variables\n",
    "    save_path = saver.save(sess,os.path.join(pathTemp,'myFinalModel.ckpt'))\n",
    "    print (\"Training duration {}\".format(trainTime))\n",
    "    totalParameters =np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()])\n",
    "    print(\"Number of training variable {}\".format(totalParameters))\n",
    "    # log\n",
    "    infoLog={}\n",
    "    infoLog[\"path\"] = path\n",
    "    infoLog[\"MSE\"] = bestNRMSETest\n",
    "    infoLog[\"num_step\"] = num_step\n",
    "    infoLog[\"num_hidden\"] = num_hidden\n",
    "    infoLog[\"num_epoch\"] = epoch\n",
    "    infoLog[\"batch_size\"] = batch_size\n",
    "    infoLog[\"maxSize\"] = maxSize\n",
    "    infoLog[\"duration\"] = trainTime\n",
    "    infoLog[\"totalParameters\"] = totalParameters\n",
    "    infoLog[\"version\"] = version\n",
    "    infoLog[\"n_layer\"] = 1\n",
    "    infoLog[\"trainDropout\"] = 0\n",
    "    infoLog[\"nameModel\"] = modelName\n",
    "    infoLog[\"conv_chan\"] = [0]\n",
    "    infoLog[\"strides\"] = 0\n",
    "    infoLog[\"conv_size\"] = 0\n",
    "    infoLog[\"amplifierName\"]=amplifierName\n",
    "    logPerf(infoLog)\n",
    "    input_nodes=[\"placeHolder/data\"]\n",
    "    output_nodes=[\"FCPred/Tanh\"]\n",
    "    optimizeGraph(pathTemp,input_nodes,output_nodes) \n",
    "    \n",
    "    ################################################\n",
    "    #   validation dataset and emulate guitar signal\n",
    "    ################################################\n",
    "    restorePath = os.path.join(pathTemp,'myBestModel.ckpt') # example for restore a previous model\n",
    "    saver.restore(sess,restorePath)\n",
    "    matrixVal = sio.loadmat(fileNameValidation)\n",
    "    matrixVal = matrixVal['val']  \n",
    "    valSize = 6*44100\n",
    "    if valSize == 0 :\n",
    "        valSize = len(matrixVal)\n",
    "    # shape validation testpLast\n",
    "    val_input,val_output = loadValidationGain(matrixVal,num_step,valSize)\n",
    "    lPrediction = []\n",
    "    lTarget = []\n",
    "    ptr3 = 0\n",
    "    no_of_batchesVal = int(np.floor((len(val_input))/batch_size))\n",
    "    for k in range(no_of_batchesVal):\n",
    "        pPrediction,pTarget = sess.run([prediction,target],{data: val_input[ptr3:ptr3+batch_size], target: val_output[ptr3:ptr3+batch_size]}) \n",
    "        lPrediction.append(pPrediction)\n",
    "        lTarget.append(pTarget)   \n",
    "        ptr3+=batch_size\n",
    "    #plt.show()scree\n",
    "    predictionArray = np.array(lPrediction,dtype=np.float32).ravel()\n",
    "    targetArray = np.array(lTarget,dtype=np.float32).ravel()\n",
    "    scipy.io.wavfile.write(os.path.join(path,'prediction.wav'),44100,predictionArray)\n",
    "    scipy.io.wavfile.write(os.path.join(path,'target.wav'),44100,targetArray)\n",
    "\n",
    "    # save emulation in a pickle format\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(predictionArray[6000:8000],label='prediction')\n",
    "    ax.plot(targetArray[6000:8000],label='target')\n",
    "    ax.legend()\n",
    "    plt.xlabel('sample n')\n",
    "    plt.ylabel('Amplitude y[n]')\n",
    "    nameFigEstimation = os.path.join(path,\"targetVsPrediction.pickle\")\n",
    "    pickle.dump(ax,open(nameFigEstimation, 'wb'))\n",
    "print (\"done, good job kids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from show import *\n",
    "%matplotlib notebook\n",
    "showPickle(nameFigEstimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('Codes')\n",
    "amplifierName = 'EnglDisto'\n",
    "fileNameTrain = 'Datasets/training'+amplifierName+'.mat'             #dataset train/test path\n",
    "fileNameTest = 'Datasets/test'+amplifierName+'.mat' # dataset validation path\n",
    "fileNameValidation = 'Datasets/val'+amplifierName+'.mat'\n",
    "maxSize = 44100\n",
    "#############################\n",
    "# Loading data\n",
    "#############################\n",
    "matrix = sio.loadmat(fileNameTrain)\n",
    "matrixTrain=matrix['train']\n",
    "matrixIn = matrixTrain[:maxSize,0]\n",
    "matrixOut = matrixTrain[:maxSize,1]\n",
    "matrixParameters = matrixTrain[:maxSize,2]\n",
    "my_indices = np.arange(len(matrixIn)-(num_step-1)) # all indices - numstep\n",
    "indices = (np.arange(num_step) +my_indices[:,np.newaxis]) # each sequence of num_step decayed from one sample\n",
    "trainInput = np.stack((np.take(matrixIn,indices),np.take(matrixParameters,indices)),axis=2) # [numsample,num_step,num_features]\n",
    "trainOutput = np.reshape(matrixOut[num_step-1:],(len(matrixIn)-(num_step-1),1))\n",
    "trainOutput[1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mat = np.arange(143000)\n",
    "numstep = 51\n",
    "batchsize = 333\n",
    "numtest = len(mat)\n",
    "no_of_batchesTest = int(np.floor(numtest/batchsize))\n",
    "subsize= 6\n",
    "index = 0\n",
    "sizeSubTest = int(np.floor(no_of_batchesTest/subsize))\n",
    "ptr2=0\n",
    "for k in range(no_of_batchesTest):\n",
    "    if k%np.ceil(no_of_batchesTest/subsize) == 0 : # divide no_of_batches in subsize subBatches\n",
    "        print(\"ptrstart {}, ptrstop {}\".format((sizeSubTest)*index*batchsize,batchsize*((sizeSubTest)*index+sizeSubTest)-1))\n",
    "        index +=1\n",
    "        ptr2=0\n",
    "    if ((ptr2+batchsize) < batchsize*sizeSubTest-1):\n",
    "        print(\"subbatchIndice: {}/{}\".format(ptr2,ptr2+batchsize))\n",
    "        ptr2+=batchsize\n",
    "print(\"subsizeTest {}\".format(sizeSubTest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
