{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn simple layer of RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add conv layer\n",
    "add l2 regularizer\n",
    "add LSTMBlockFused\n",
    "remove dropout to add optimized graph, add quantized graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version 1.13.1 of tensorflow\n",
      "shape input train (43951, 150)\n",
      "Data loaded\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "WARNING:tensorflow:From <ipython-input-1-df9897522320>:111: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [13,1] vs. [2500,1]\n\t [[node sub (defined at <ipython-input-1-df9897522320>:137) ]]\n\t [[node Mean (defined at <ipython-input-1-df9897522320>:137) ]]\n\nCaused by op 'sub', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-df9897522320>\", line 137, in <module>\n    MSE = tf.reduce_mean(tf.square(prediction-target))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 812, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 9536, in sub\n    \"Sub\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [13,1] vs. [2500,1]\n\t [[node sub (defined at <ipython-input-1-df9897522320>:137) ]]\n\t [[node Mean (defined at <ipython-input-1-df9897522320>:137) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [13,1] vs. [2500,1]\n\t [[{{node sub}}]]\n\t [[{{node Mean}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df9897522320>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumTrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;31m# This is to have a train summary and a test summary of the same size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpMSETrainTemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpEnergyTargetTemp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpVal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEnergyTarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0mpMSETrain\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpMSETrainTemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [13,1] vs. [2500,1]\n\t [[node sub (defined at <ipython-input-1-df9897522320>:137) ]]\n\t [[node Mean (defined at <ipython-input-1-df9897522320>:137) ]]\n\nCaused by op 'sub', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-1-df9897522320>\", line 137, in <module>\n    MSE = tf.reduce_mean(tf.square(prediction-target))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\", line 812, in binary_op_wrapper\n    return func(x, y, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_math_ops.py\", line 9536, in sub\n    \"Sub\", x=x, y=y, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [13,1] vs. [2500,1]\n\t [[node sub (defined at <ipython-input-1-df9897522320>:137) ]]\n\t [[node Mean (defined at <ipython-input-1-df9897522320>:137) ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('Codes')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from dataShaping import *\n",
    "from savePerf import *\n",
    "from saveTransformedGraph import optimizeGraph\n",
    "import scipy.io.wavfile\n",
    "import time\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.rnn import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "modelName = \"RNN1\"\n",
    "# create directory experiment\n",
    "date = time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "path = os.path.join(\"Experiments\",date)\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "    #experiment/\"date\"/temp will contain the backuped model parameters\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "    # if you run the file two time in a minute\n",
    "else :\n",
    "    date = date+'(2)'\n",
    "    path = os.path.join(\"Experiments\",date)\n",
    "    os.makedirs(path)\n",
    "    pathTemp = os.path.join(path,'temp')\n",
    "    os.makedirs(pathTemp)\n",
    "\n",
    "# directory that will contain tensorboard information\n",
    "pathLog = 'Tf_logs'\n",
    "if not os.path.isdir(pathLog):\n",
    "    os.makedirs(pathLog)\n",
    "pathLog = \"{}/run-{}/\".format(pathLog,date)\n",
    "\n",
    "version = tf.__version__\n",
    "print (\"version {} of tensorflow\".format(version))\n",
    "\n",
    "#############################\n",
    "# Model parameters\n",
    "#############################\n",
    "trainTestRatio = 0.8\n",
    "#if you cannot load all the data set in Ram specify wich part you want to load (0 means all the dataset)\n",
    "maxSize = 0\n",
    "num_step = 150                                       # time step before reduction\n",
    "conv_chan = [35]                                         #number of kernel for convolution\n",
    "conv_strides = 3#int(np.ceil(num_step/num_LSTM))                  #decay between two convolution\n",
    "conv_size = 12                                         #filter size for the convolution\n",
    "size_poll = 4\n",
    "reg_scale = 0\n",
    "l1l2Prop = 0.4  # 1 =>l1, 0=> l2\n",
    "reg_scale_l1 = l1l2Prop*reg_scale\n",
    "reg_scale_l2 = ((1-l1l2Prop)/2)*reg_scale\n",
    "num_hidden = 150                                      #num of hidden units\n",
    "num_class = 1                                          #size of the output\n",
    "num_feature = 1                                        # size of the input\n",
    "batch_size = 2500                                  # number of sequence taken before to compute the gradient\n",
    "n_layer = 1                                             #num_layer\n",
    "\n",
    "#num_hidden = num_hidden/keep_prob\n",
    "num_epoch = 1                                      # process all the datas num_epoch times\n",
    "trainDuration = 60*60*15                             # or during a determined duration(second)\n",
    "amplifierName = 'EnglDisto'\n",
    "fileNameTrain = 'Datasets/training'+amplifierName+'.mat'             #dataset train/test path\n",
    "fileNameTest = 'Datasets/test'+amplifierName+'.mat' # dataset validation path\n",
    "fileNameValidation = 'Datasets/val'+amplifierName+'.mat'\n",
    "\n",
    "#############################\n",
    "# Loading data\n",
    "#############################\n",
    "matrix = sio.loadmat(fileNameTrain)\n",
    "matrixTrain = matrix['train']\n",
    "matrix = sio.loadmat(fileNameTest)\n",
    "matrixTest = matrix['test']\n",
    "if maxSize ==0:\n",
    "    maxSize = len(matrixTrain)\n",
    "    print(\"maxSize = {}\".format(maxSize))\n",
    "\n",
    "train_input,train_output,test_input,test_output = loadInputOutputSeq(matrixTrain,matrixTest,num_step,maxSize)\n",
    "\n",
    "\n",
    "print(\"shape input train {}\".format(np.shape(train_input)))\n",
    "numTrain = len(train_output)\n",
    "print (\"Data loaded\")\n",
    "#######################\n",
    "#Graph\n",
    "#######################\n",
    "\n",
    "G = tf.Graph()\n",
    "with G.as_default():\n",
    "    with tf.name_scope(\"placeHolder\"):\n",
    "        data = tf.placeholder(tf.float32, [None, num_step], name =\"data\") #Number of examples, number of input step (time step), dimension of each input\n",
    "        target = tf.placeholder(tf.float32, [None, num_class],name = \"target\") # batchSize, nbClass\n",
    "\n",
    "    dataShaped = tf.reshape(data,[tf.shape(data)[0],num_step,1,1]) # batchSize, num_step width channel\n",
    "    with tf.variable_scope(\"ConvLayers\"):\n",
    "        regularizerC1 = tf.contrib.layers.l1_l2_regularizer(scale_l1=reg_scale_l1,scale_l2=reg_scale_l2,scope=\"regC1\")\n",
    "        dataReduced = tf.layers.conv2d(inputs = dataShaped,filters = conv_chan[0],\n",
    "                                       kernel_size = (conv_size,1),strides=(4,1),\n",
    "                                       padding = \"same\",activation=tf.nn.elu,kernel_regularizer=regularizerC1,name=\"C1\")#batch_size num_Lstm num_channel\n",
    "        regularizerC2 = tf.contrib.layers.l1_l2_regularizer(scale_l1=reg_scale_l1,scale_l2=reg_scale_l2,scope=\"regC2\")\n",
    "        dataReduced = tf.layers.conv2d(inputs = dataReduced,filters = conv_chan[0],\n",
    "                                       kernel_size = (conv_size,1),strides=(3,1),\n",
    "                                       padding = \"same\",activation=tf.nn.elu,kernel_regularizer=regularizerC2,name=\"C2\")#batch_size num_Lstm num_channel\n",
    "        \n",
    "    dataReduced = tf.reshape(dataReduced,[tf.shape(data)[0],tf.shape(dataReduced)[1],conv_chan[0]]) #NHC\n",
    "    \n",
    "    fusedCell = tf.contrib.rnn.BasicRNNCell(num_hidden)\n",
    "    fusedCellAdaptator = tf.contrib.rnn.FusedRNNCellAdaptor(fusedCell,use_dynamic_rnn=True)\n",
    "\n",
    "    dataReduced = tf.transpose(dataReduced,[1,0,2]) #HNC\n",
    "\n",
    "    with tf.name_scope(\"extractLastValueLSTM\"):\n",
    "        val,states = fusedCellAdaptator(dataReduced,initial_state=tf.zeros([tf.shape(data)[0],num_hidden], dtype=tf.float32)) # val dim is [num_step,batch_size, numhidden]\n",
    "        \n",
    "    # Let's first fetch the last index of seq length\n",
    "    # last_index would have a scalar value\n",
    "        last_index = tf.shape(val)[0] - 1\n",
    "        last = tf.gather(val,last_index)\n",
    "\n",
    "   \n",
    "    prediction = fully_connected(last,int(target.get_shape()[1]),activation_fn=tf.nn.tanh,weights_regularizer=None,scope=\"FCPred\")\n",
    "    #Compute the mean square error\n",
    "    MSE = tf.reduce_mean(tf.square(prediction-target))\n",
    "    EnergyTarget = tf.reduce_mean(tf.square(target)) \n",
    "    \n",
    "    #get regularizer\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    MSEReg = tf.add_n([MSE]+reg_losses,name=\"MSEReg\")\n",
    "    # create optimizer\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    #Compute gradient and apply backpropagation\n",
    "    minimize = optimizer.minimize(MSEReg)\n",
    "\n",
    "    # Create summary view for tensorboard\n",
    "    mse_summary = tf.summary.scalar('RMSE',tf.sqrt(MSE))\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    #Create an init op to initialize variable\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver() # save variable, use saver.restore(sess,\"date/tmp/my_model.ckpt\") instead of sess.run(init_op)\n",
    "\n",
    "##############################\n",
    "# Execution du graphe\n",
    "##############################\n",
    "    \n",
    "with tf.Session(graph=G) as sess:\n",
    "    #restorePath = os.path.join('2017-09-11-18-07','temp','my_model.ckpt')\n",
    "    #saver.restore(sess,restorePath)\n",
    "    sess.run(init_op)\n",
    "    train_writer = tf.summary.FileWriter(pathLog+'train',graph =tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(pathLog+'test')\n",
    "    \n",
    "    no_of_batches = int(np.floor((numTrain)/batch_size)) # numtrain -numstep    no_of_batchesTest = int(np.floor((len(test_input)-num_step)/batch_size))\n",
    "    no_of_batchesTest = int(np.floor((len(test_input))/batch_size))\n",
    "    tStart = time.clock()\n",
    "    epoch =0\n",
    "    NRMSETest = 10\n",
    "    bestNRMSETest = 1    \n",
    "    # train until the number of epoch or the training time is reached\n",
    "    for epoch in range(num_epoch):\n",
    "        tEpoch = time.clock()\n",
    "        if (time.clock()-tStart < trainDuration) :\n",
    "            ptr = 0\n",
    "            if epoch % 20==0 : # each twenty epochs save the model\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myGraph.pb',as_text=False)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myModel.ckpt'))\n",
    "            \n",
    "            pMSETrain=0\n",
    "            pEnergyTarget=0\n",
    "            for j in range(no_of_batches):\n",
    "                inp, out = train_input[ptr:ptr+batch_size],train_output[ptr:ptr+batch_size]\n",
    "                ptr+=batch_size\n",
    "                \n",
    "                if j % np.floor(numTrain/len(test_input)) ==0 : # This is to have a train summary and a test summary of the same size\n",
    "                    _,summary_str,pMSETrainTemp,pEnergyTargetTemp = sess.run([minimize,summary_op,MSE,EnergyTarget],{data: inp, target: out})\n",
    "                    #print(\"shape:{}\".format(numTrain/len(test_input)))\n",
    "                    pMSETrain += pMSETrainTemp\n",
    "                    pEnergyTarget += pEnergyTargetTemp\n",
    "                    step = epoch*no_of_batches+j\n",
    "                    # save the training RMSE for tensorboard\n",
    "                    train_writer.add_summary(summary_str,step)                   \n",
    "\n",
    "                else :\n",
    "                    _,pMSETrainTemp,pEnergyTargetTemp = sess.run([minimize,MSE,EnergyTarget],{data: inp, target: out})\n",
    "                    pMSETrain += pMSETrainTemp\n",
    "                    pEnergyTarget += pEnergyTargetTemp\n",
    "                    #[print(n.name) for n in tf.get_default_graph().as_graph_def().node]\n",
    "            # compute an estimation of the RMSE for this epoch       \n",
    "            MSETrain = pMSETrain/no_of_batches\n",
    "            EnergyTargetTrain = pEnergyTarget/no_of_batches\n",
    "            NRMSETrain = np.sqrt(MSETrain/EnergyTargetTrain)\n",
    "            print (\"Epoch -{} calculated in {:5.2f} s \".format(epoch,time.clock()-tEpoch))\n",
    "            # evaluate the model on the test set \n",
    "            pMSE = 0\n",
    "            ptr2 = 0\n",
    "            pEnergyTarget = 0\n",
    "            for k in range(no_of_batchesTest):\n",
    "                pMSETemp,pEnergyTargetTemp,summary_str = sess.run([MSE,EnergyTarget,summary_op],{data: test_input[ptr2:ptr2+batch_size] , target: test_output[ptr2:ptr2+batch_size]})\n",
    "                pMSE += pMSETemp\n",
    "                ptr2 += batch_size\n",
    "                pEnergyTarget+=pEnergyTargetTemp\n",
    "                step = epoch*no_of_batchesTest+k\n",
    "                test_writer.add_summary(summary_str,step*np.floor(numTrain/len(test_input)))\n",
    "            MSETest = pMSE/no_of_batchesTest\n",
    "            EnergyTargetTest = pEnergyTarget/no_of_batchesTest\n",
    "            NRMSETest = np.sqrt(MSETest/EnergyTargetTest)\n",
    "            if NRMSETest<bestNRMSETest:\n",
    "                bestNRMSETest=NRMSETest\n",
    "                tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myBestGraph.pbtxt',as_text=True)\n",
    "                save_path = saver.save(sess,os.path.join(pathTemp,'myBestModel.ckpt'))\n",
    "            print(\"Epoch {}, NRMSE Test/best: {:.5f}/{:.5f}, Training NRMSE: {:.5f}, deviation of {:.2f}%\".format(epoch,NRMSETest,bestNRMSETest,NRMSETrain,100*np.sqrt((NRMSETrain-NRMSETest)**2)/NRMSETrain))\n",
    "        else : break # break the while loop if number of epoch is reached\n",
    "    tStop = time.clock()\n",
    "    trainTime = time.strftime(\"%d:%H:%M:%S \", time.gmtime(tStop-tStart))\n",
    "     \n",
    "    #######################\n",
    "    # Save Graph variable and information about the running session\n",
    "    #######################\n",
    "    # save graph model\n",
    "    tf.train.write_graph(sess.graph_def,\"{}/\".format(pathTemp),'myFinalGraph.pbtxt',as_text=True)\n",
    "    # Save checkpoint variables\n",
    "    save_path = saver.save(sess,os.path.join(pathTemp,'myFinalModel.ckpt'))\n",
    "    print (\"Training duration {}\".format(trainTime))\n",
    "    totalParameters =np.sum([np.product([xi.value for xi in x.get_shape()]) for x in tf.trainable_variables()])\n",
    "    print(\"Number of training variable {}\".format(totalParameters))\n",
    "    # log\n",
    "    infoLog={}\n",
    "    infoLog[\"path\"] = path\n",
    "    infoLog[\"MSE\"] = bestNRMSETest\n",
    "    infoLog[\"num_step\"] = num_step\n",
    "    infoLog[\"num_hidden\"] = num_hidden\n",
    "    infoLog[\"num_epoch\"] = epoch\n",
    "    infoLog[\"batch_size\"] = batch_size\n",
    "    infoLog[\"maxSize\"] = maxSize\n",
    "    infoLog[\"duration\"] = trainTime\n",
    "    infoLog[\"totalParameters\"] = totalParameters\n",
    "    infoLog[\"version\"] = version\n",
    "    infoLog[\"n_layer\"] = n_layer\n",
    "    infoLog[\"trainDropout\"] = 0\n",
    "    infoLog[\"nameModel\"] = modelName\n",
    "    infoLog[\"conv_chan\"] = conv_chan\n",
    "    infoLog[\"strides\"] = conv_strides\n",
    "    infoLog[\"conv_size\"] = conv_size\n",
    "    infoLog[\"amplifierName\"]=amplifierName\n",
    "    logPerf(infoLog)\n",
    "    input_nodes=[\"placeHolder/data\"]\n",
    "    output_nodes=[\"FCPred/Tanh\"]\n",
    "    optimizeGraph(pathTemp,input_nodes,output_nodes)\n",
    "    \n",
    "    \n",
    "                                                 \n",
    "    \n",
    "    ###############################\n",
    "    #   validation dataset and emulate guitar signal\n",
    "    ###############################\n",
    "    matrixVal = sio.loadmat(fileNameValidation)\n",
    "    matrixVal = matrixVal['val']  \n",
    "    valSize = 0\n",
    "    if valSize == 0 :\n",
    "        valSize = len(matrixVal)\n",
    "    # shape validation test\n",
    "    val_input,val_output = loadValidationSeq(matrixVal,num_step,valSize)\n",
    "    lPrediction = []\n",
    "    lTarget = []\n",
    "    ptr3 = 0\n",
    "    no_of_batchesVal = int(np.floor((len(val_input))/batch_size))\n",
    "    for k in range(no_of_batchesVal):\n",
    "        pPrediction,pTarget = sess.run([prediction,target],{data: val_input[ptr3:ptr3+batch_size], target: val_output[ptr3:ptr3+batch_size]}) \n",
    "        lPrediction.append(pPrediction)\n",
    "        lTarget.append(pTarget)   \n",
    "        ptr3+=batch_size\n",
    "    #plt.show()scree\n",
    "    predictionArray = np.array(lPrediction,dtype=np.float32).ravel()\n",
    "    targetArray = np.array(lTarget,dtype=np.float32).ravel()\n",
    "    scipy.io.wavfile.write(os.path.join(path,'prediction.wav'),44100,predictionArray)\n",
    "    scipy.io.wavfile.write(os.path.join(path,'target.wav'),44100,targetArray)\n",
    "\n",
    "    # save emulation in a pickle format\n",
    "    ax = plt.subplot(111)\n",
    "    ax.plot(predictionArray[:10000],label='prediction')\n",
    "    ax.plot(targetArray[:10000],label='target')\n",
    "    ax.legend()\n",
    "    nameFigEstimation = os.path.join(path,\"targetVsPrediction.pickle\")\n",
    "    pickle.dump(ax,open(nameFigEstimation, 'wb'))\n",
    "print (\"done, good job kids\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from show import *\n",
    "%matplotlib notebook\n",
    "showPickle(nameFigEstimation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
